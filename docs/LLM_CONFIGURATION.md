# ğŸ¤– ConfiguraÃ§Ã£o do LLM (Large Language Model)

## ğŸ“‹ **VisÃ£o Geral**

O **DocCollab Assistant** pode operar em trÃªs modos:

1. **ğŸ”Œ OFFLINE** (padrÃ£o): 100% baseado em regras, sem dependÃªncias externas
2. **ğŸ”€ HYBRID**: Combina regras + LLM para casos complexos
3. **ğŸŒ LLM_ONLY**: Usa apenas LLM externo

---

## ğŸš€ **Modo OFFLINE (PadrÃ£o)**

**Sem configuraÃ§Ã£o necessÃ¡ria!** O sistema funciona completamente offline com:

- âœ… DetecÃ§Ã£o de intenÃ§Ãµes por palavras-chave
- âœ… SugestÃµes baseadas em regras
- âœ… Suporte completo a PT/EN/ES
- âœ… Zero custos de API
- âœ… Zero latÃªncia de rede
- âœ… 100% privado

### Funcionalidades DisponÃ­veis

```python
# Comandos reconhecidos automaticamente:
- "olÃ¡" / "hello" / "hola" â†’ SaudaÃ§Ã£o
- "ajuda" / "help" / "ayuda" â†’ Lista de funcionalidades
- "compilar" / "compile" â†’ SugestÃ£o de compilaÃ§Ã£o
- "bibtex" / "citaÃ§Ã£o" â†’ SugestÃ£o de geraÃ§Ã£o BibTeX
- "lint" / "erros" / "verificar" â†’ SugestÃ£o de lint
```

---

## ğŸŒ **Habilitar LLM Externo**

### **1. Configurar VariÃ¡veis de Ambiente**

#### **Windows (PowerShell)**
```powershell
$env:LLM_PROVIDER = "openai"
$env:LLM_API_KEY = "sk-..."
$env:LLM_MODE = "hybrid"  # ou "llm_only"
```

#### **Linux/macOS (Bash)**
```bash
export LLM_PROVIDER=openai
export LLM_API_KEY=sk-...
export LLM_MODE=hybrid  # ou llm_only
```

#### **Arquivo `.env`** (recomendado)
```ini
# .env
LLM_PROVIDER=openai
LLM_API_KEY=sk-your-api-key-here
LLM_MODE=hybrid

# Opcional
LLM_MODEL=gpt-4
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=500
```

### **2. Provedores Suportados**

| Provedor | Valor `LLM_PROVIDER` | Requer API Key |
|----------|---------------------|----------------|
| **OpenAI** | `openai` | Sim (`sk-...`) |
| **Anthropic** | `anthropic` | Sim (`sk-ant-...`) |
| **Cohere** | `cohere` | Sim |
| **Local** | `local` | NÃ£o (Llama, etc.) |
| **Offline** | `none` | NÃ£o (padrÃ£o) |

### **3. Modos de OperaÃ§Ã£o**

#### **ğŸ”Œ OFFLINE** (`LLM_MODE=offline`)
```
UsuÃ¡rio â†’ Regras â†’ Resposta
```
- Sem custos
- Sem latÃªncia de rede
- Respostas instantÃ¢neas
- Funcionalidade limitada

#### **ğŸ”€ HYBRID** (`LLM_MODE=hybrid`) â­ **Recomendado**
```
UsuÃ¡rio â†’ Regras (confianÃ§a < 70%) â†’ LLM â†’ Resposta
         â†“ (confianÃ§a â‰¥ 70%)
         Resposta
```
- Melhor custo-benefÃ­cio
- Usa regras para casos simples
- LLM apenas para casos complexos
- Fallback automÃ¡tico

#### **ğŸŒ LLM_ONLY** (`LLM_MODE=llm_only`)
```
UsuÃ¡rio â†’ LLM â†’ Resposta
```
- Respostas mais naturais
- Maior custo
- Requer conexÃ£o
- Sem fallback

---

## ğŸ”§ **ImplementaÃ§Ã£o de Provedores**

### **OpenAI (GPT-4)**

1. **Instalar dependÃªncia:**
```bash
pip install openai
```

2. **Configurar `.env`:**
```ini
LLM_PROVIDER=openai
LLM_API_KEY=sk-...
LLM_MODEL=gpt-4
```

3. **Implementar em `llm_client.py`:**
```python
if self.provider == LLMProvider.OPENAI:
    import openai
    openai.api_key = self.api_key
    
    response = openai.ChatCompletion.create(
        model=os.getenv('LLM_MODEL', 'gpt-4'),
        messages=[
            {
                "role": "system", 
                "content": f"You are a LaTeX assistant. Respond in {language}."
            },
            {
                "role": "user", 
                "content": user_message
            }
        ],
        temperature=float(os.getenv('LLM_TEMPERATURE', 0.7)),
        max_tokens=int(os.getenv('LLM_MAX_TOKENS', 500))
    )
    
    return {
        'text': response.choices[0].message.content,
        'actions': self._parse_actions(response),
        'source': 'llm',
        'confidence': 1.0
    }
```

### **Anthropic (Claude)**

1. **Instalar:**
```bash
pip install anthropic
```

2. **Configurar `.env`:**
```ini
LLM_PROVIDER=anthropic
LLM_API_KEY=sk-ant-...
LLM_MODEL=claude-3-opus-20240229
```

### **Modelos Locais (Llama, etc.)**

1. **Instalar:**
```bash
pip install llama-cpp-python
```

2. **Configurar `.env`:**
```ini
LLM_PROVIDER=local
LLM_MODEL_PATH=/path/to/model.gguf
```

---

## ğŸ“Š **Monitoramento**

### **Verificar Status do LLM**

```python
from services.llm_client import get_llm_client

client = get_llm_client()

print(f"Provider: {client.provider}")
print(f"Mode: {client.mode}")
print(f"Enabled: {client.is_enabled()}")
```

### **Logs de Uso**

Todos os usos do LLM sÃ£o registrados em `logs/audit_YYYY-MM-DD.jsonl`:

```json
{
  "timestamp": "2025-10-07T12:34:56Z",
  "event_type": "assistant_message",
  "user_id": 1,
  "action": "LLM response",
  "details": {
    "provider": "openai",
    "mode": "hybrid",
    "tokens_used": 150
  }
}
```

---

## ğŸ”’ **SeguranÃ§a**

### **Boas PrÃ¡ticas**

1. âœ… **Nunca** commitar `.env` com API keys
2. âœ… Usar variÃ¡veis de ambiente em produÃ§Ã£o
3. âœ… Rotacionar API keys periodicamente
4. âœ… Monitorar custos de uso
5. âœ… Configurar rate limiting

### **Arquivo `.gitignore`**
```gitignore
.env
*.env
.env.local
logs/
```

### **Limites Recomendados**

```ini
# .env
LLM_MAX_TOKENS=500  # Evitar respostas muito longas
LLM_TEMPERATURE=0.7  # EquilÃ­brio entre criatividade e precisÃ£o
LLM_RATE_LIMIT=10  # MÃ¡ximo 10 requisiÃ§Ãµes/minuto
```

---

## ğŸ§ª **Testes**

### **Testar Modo Offline**
```bash
# Sem variÃ¡veis de ambiente
python -c "from services.llm_client import get_llm_client; print(get_llm_client().mode)"
# Output: LLMMode.OFFLINE
```

### **Testar com LLM**
```bash
export LLM_API_KEY=sk-test
export LLM_PROVIDER=openai
python -c "from services.llm_client import get_llm_client; print(get_llm_client().is_enabled())"
# Output: True
```

### **Testar Resposta**
```python
from services.llm_client import get_llm_client

client = get_llm_client()
response = client.generate_response(
    "Como compilar meu documento?",
    language='pt'
)
print(response['text'])
```

---

## ğŸ“ˆ **Custos Estimados**

### **OpenAI GPT-4**
- **Input**: $0.03 / 1K tokens
- **Output**: $0.06 / 1K tokens
- **MÃ©dia/resposta**: ~300 tokens = **$0.015**

### **Modo Hybrid (Recomendado)**
- **Regras**: 90% das requisiÃ§Ãµes (grÃ¡tis)
- **LLM**: 10% das requisiÃ§Ãµes
- **Custo mensal** (1000 msgs): ~$1.50

### **Modo Offline**
- **Custo**: $0 ğŸ’°
- **100% gratuito**

---

## â“ **FAQ**

### **P: O que acontece se a API cair?**
**R:** No modo `hybrid`, o sistema automaticamente volta para regras. No modo `llm_only`, retorna erro.

### **P: Posso usar mÃºltiplos provedores?**
**R:** Atualmente nÃ£o, mas vocÃª pode alternar mudando `LLM_PROVIDER`.

### **P: Como desabilitar completamente o LLM?**
**R:** Remova ou comente `LLM_API_KEY` no `.env`. O sistema volta automaticamente para OFFLINE.

### **P: O modo offline Ã© suficiente?**
**R:** Sim! Para 90% dos casos, as regras sÃ£o suficientes e mais rÃ¡pidas.

---

## ğŸ“ **PrÃ³ximos Passos**

1. âœ… Implementar provider OpenAI
2. âœ… Adicionar rate limiting
3. âœ… Implementar cache de respostas
4. âœ… Adicionar suporte a funÃ§Ã£o calling (tools)
5. âœ… Implementar fallback automÃ¡tico

---

## ğŸ¤ **Contribuindo**

Para adicionar um novo provedor:

1. Adicionar em `LLMProvider` enum
2. Implementar em `_llm_response()`
3. Documentar neste arquivo
4. Adicionar testes

---

## ğŸ“ **Suporte**

- ğŸ“§ Email: support@doccollab.com
- ğŸ“š Docs: https://docs.doccollab.com
- ğŸ’¬ Discord: https://discord.gg/doccollab



